# item9. 웹 크롤러 설계
크롤러 이용
- 검색 엔진 인덱싱: 검색 엔진을 위한 로컬 인덱스를 만듬
- 웹 아카이빙: 나중에 사용할 목적으로 장기보관 하기 위해 웹에서 정보를 모음
- 웹 마이닝: 인터넷의 유용한 지식을 도출
- 웹 모니터링: 인터넷에서 저작권이나 상표권이 침해되는 사례 모니터링

## 1. 문제 이해 및 설계 범위 확정
예시)
- URL들이 가르키는 모든 웹 페이지 다운로드
- 다운받은 웹 페이지에서 모든 URL 추출
- 추출된 URL들을 다운로드 할 URL 목록에 추가하고 1번부터 다시 반복

좋은 웹 크롤러
- 규모 확장성: 병행성 활용
- 안정성: 잘못 작성된 HTML, 404서버, 장애, 악성 코드가 붙어있는 링크에 대응
- 예절: 웹 사이트에 짧은 시간 동안 너무 많은 요청 보내지 않기
- 확장성: 새로운 형태의 콘텐츠 지원

개략적 추정
- 매달 10억개의 웹 페이지 다운로드
- QPS(Query per second): 10억/30일/24시간/3600초 = 400QPS
- 최대(Peak) QPS: 2*QPS= 800QPS
- 웹 페이지의 평균 크기: 500k
- 데이터: 10억 페이지 * 500k = 500TB
- 10년간 운영하면, 500TB * 12 * 5 = 30PB

## 2. 개략적 설계안 제시 및 동의 구하기
- 시작 URL 집합
- 미수집 URL 저장소
- HTML 다운로더
  - 도메일 이름 변환기
- 컨텐츠 파서
- 중복 컨텐츠 확인
  - 컨텐츠 저장소 db
- URL 추출기
- URL 필터
- 방문된 URL 확인
- 미수집 URL 저장소 반복

### 시작 URL 집합
웹 크롤러가 크롤링을 시작하는 출발점
- 도메인 이름이 붙은 모든 페이지의 URL을 시작 URL로 사용

일반적으로 전체 URL 공간을 작은 부분집합으로 나누는 전략 사용
- 지역적인 특색
- 나라별
- 주제별

### 미수집 URL 저장소
크롤링 상태
- 다운로드할 URL
- 다운로드된 URL
로 나뉘는데,  
다운로드 될 URL을 저장 및 관리하는 컴포넌트이다.

### HTML 다운로더
인터넷에서 웹 페이지를 다운로드하는 컴포넌트  
다운로드할 페이징는 미수집 URL 저장소가 제공

### 도메인 이름 변환기
URL을 IP주소로 변환

### 컨텐츠 파서
웹 페이지를 다운로드 한 뒤, 파싱과 검증 절차를 거쳐야 한다.  
ㄴ 저장 공간 낭비하지 않기 위함과 이상한 웹 페이지일 수 있음  
- 크롤링 서버에 있으면, 크롤링 과정이 느려지기 때문에, 독립된 컴포넌트

### 중복 컨텐츠 확인
데이터 중복을 줄이고, 데이터 처리에 소요되는 시간을 줄이기 위함.  
- 간단한 방법: 두 HTML 문서를 비교 / 느리고 비효율적

추천방법: 해시 함수를 사용하여, 두 HTML 문서의 해시 값을 비교

### 컨텐츠 저장소
HTML 문서를 보관하는 시스템  
- 저장할 데이터의 유형
- 크기
- 저장소 접근 빈도
- 데이터의 유효 기간

보통의 구현 방법
- 데이터 양이 너무 많으니, 대부분의 컨텐츠는 디스크
- 인기 있는 컨텐츠는 메모리에 두어 접근 지연시간 줄임

### URL 추출기
HTML페이지를 파싱하여, 링크 추출
- 상대 경로는 절대경로로 변환

### URL 필터
- 특정한 컨텐츠 타입이나 파일 확장자를 갖는 URL
- 접속 시 오류가 발생하는 URL
- 접근 제외 목록에 포함된 URL
을 배제하는 역할

### 방문된 URL 확인
블룸 필터나 해시테이블을 사용

### URL 저장소
이미 방문한 저장소를 보관하는 저장소

## 3. 상세 설계
### DFS vs BFS
dfs는 그래프의 크기가 클 경우 깊이를 가늠하지 못 해서,  
보통 BFS 사용

문제점
- 한 페이지에서 나오는 링크의 상당수는 같은 서버로 되돌아 감
  - 병렬로 처리되면, 한 웹페이지에 많은 요청을 보냄 - 예의 없는 크롤러가 됨
- URL 간에 우선순위를 두지 않음

### 미수집 URL 저장소
미수집 URL 저장소를 활용하여, 문제들을 쉽게 해결

#### 예의  
웹 크롤러는 수집 대상 서버로 짧은 시간에 많은 요청을 보내면 안됨.

지켜야 할 원칙: 동일 웹사이트에 대해서는 한 번에 한 페이지만 요청
- 웹 사이트의 호스트명과 다운로드를 수행하는 작업 스레드 사이의 관계 유지
- 설계
  - 큐 라우터: 같은 호스트에 속한 URL은 언제나 같은 큐(호스트)로 가도록 보장
  - 매핑 테이블: 호스트 이름과 큐 사이의 관계를 보관하는 테이블
  - 큐: 같은 호스트에 속한 URL은 같은 큐에 보관
  - 큐 선택기: 큐들을 순회하며, 해당 큐에서 나온 URL을 다운로드
  - 작업 스레드: 전달된 URL을 다운로드 하는 작업

#### 우선순위
크롤러 입장에서는 중요한 페이지를 먼저 수집하는 것이 바람직함

#### 신선도
웹 페이지는 수시로 추가되고, 삭제되고, 변경됨.  
이미 다운로드 한 곳이여도, 재수집의 필요성이 있음  
전략
- 웹 페이지의 변경 이력 활용
- 우선순위를 활용하여, 중요한 페이지는 좀 더 자주 재수집

#### 미수집 URL 저장소를 위한 지속성 저장장치
대부분의 URL은 디스크에 두지만, IO비용을 줄이기 위해 메모리 버퍼에 큐를 배치

### HTML 다운로더
웹 페이지를 다운받음

#### Robots.txt
로봇 제외 프로토콜이며, 웹 크롤러가 어떤 페이지를 방문할 수 있는지 제어  
해당 파일은 캐시에 보관

#### 성능 최적화
1. 분산 크롤링
   1. 크롤링 작업을 여러 서버에 분산
2. 도메인 이름 변환 결과 캐시
   1. DNS 요청을 보내고 결과를 받는 동기적 특성 때문에, 병목이 생김
   2. DNS 조회 결과로 얻어진 도메인 이름과 IP 주소 사이의 관계를 캐시에 보관해 놓고, 배치를 돌려 주기적으로 갱신
3. 지역성
   1. 크롤링 서버를 지역별로 분산 - 페이지 다운로드 시간 감소
4. 짧은 타임아웃

#### 안정성
- 안정 해시
- 크롤링 상태 및 수집 데이터 저장
- 예외 처리
- 데이터 검증

#### 확장성
- 새로운 형태의 컨텐츠를 쉽게 지원할 수 있도록 설계

#### 문제 있는 컴텐츠 감지 및 회피
1. 중복 컨텐츠
   1. 해시나 체크섬을 사용하여, 중복 컨텐츠 탐치
2. 거미 덫(spider trap)
   1. 크롤러를 무한 루프에 빠뜨리도록 설계한 웹 페이지.
3. 데이터 노이즈
   1. 가치가 없는 컨튼체 제외

## 4. 마무리
좋은 크롤러
- 규묘 확장성
- 예의
- 확장성
- 안정성

추가로 논의 하면 좋을 것
- 서버측 렌더링
  - 링크를 즉석 에서 만드는 경우, 동적 으로 생성 되는 링크 발견 불가능 하기 때문에, 페이지 파싱 전에 서버 측 렌더링 적용
- 원치 않는 페이지 필터링
  - 스팸 방지 컴포 넌트를 사용 하여, 품질이 떨어 지거나, 스팸성 인 페이지 거르기
- db 다중화 및 샤딩
  - 데이터 계층의 가용성, 규모 확장성, 안정성 향상
- 수평정 규모 확장성
  - 무상태 서버
- 가용성, 일관성, 안정성
- 데이터 분석 솔루션
